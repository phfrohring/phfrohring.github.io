<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1"/>
        <title>Pierre-Henry Fröhring</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" type="text/css"/>
        <link rel="stylesheet" type="text/css" href="/css/style.css"/>
                <link rel="icon" type="image/png" href="/images/favicon.png"/>
                        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" /><script type="text/javascript">window.onload = function(){var mathElements = document.getElementsByClassName("math");
        for (var i=0; i < mathElements.length; i++)
        {
         var texText = mathElements[i].firstChild
         katex.render(texText.data, mathElements[i])
        }}
        </script>
            </head>

    <body>
        <div class="top-wrapper">
            <header class="top-header" title="By MartinThoma - Own work, CC0, https://commons.wikimedia.org/w/index.php?curid=45488162" style="background-image: url(/images/header-orig-deep-dream-0210.jpg);background-size: cover; background-position: center;"></header>

            <div class="columns_1">
                <nav class="navigation">
                    <ul class="list_of-predifined_publications">
                        <li class="predefined_publication"><a href="/pubs/curriculum-vitae-c5566488.html">CV</a></li>
                        <li class="predefined_publication"><a href="/">Search</a></li>
                    </ul>
                    <input class="card search_box" type="text" id="search_box" placeholder="some words   tag:some_tag">
                </nav>

                <div class="content">
                    <div class="publication-wrapper">
                                                <header>
                            <h1 class="title">Deep Learning</h1>
                                                        <p class="abstract">Notes on Deep Learning.</p>
                                                                                                                <p class="date">2017-08-03</p>
                                                                                                                      <p class="author"><a href="mailto:contact@pierrehenryfrohring.com">Pierre-Henry Fröhring</a></p>
                                                                                  </header>
                                                                        <nav id="TOC">
                            <ul>
                            <li><a href="#notations">Notations</a></li>
                            <li><a href="#heaviside-step-function">Heaviside step function</a></li>
                            <li><a href="#perceptron">Perceptron</a></li>
                            <li><a href="#supervised-learning">Supervised learning</a></li>
                            <li><a href="#deep-learning">Deep learning</a></li>
                            <li><a href="#backpropagation">Backpropagation</a></li>
                            <li><a href="#practice">Practice</a></li>
                            </ul>
                        </nav>
                                                <h1 id="notations">Notations</h1>
<ul>
<li>‘abcdfegh ≡ X’ ⇔ ‘abcdfegh’ a.k.a. ‘X’.</li>
<li>‘A;B;C;… ⊢ E;F;G;…’ ≡ From assumptions A B C … we can conclude E F G ….</li>
<li>An ordered list ≡ [a,b,c, …, z].</li>
<li>An unordered list ≡ [a b c … z].</li>
<li>An ordered set ≡ {a,b,c,…,z}.</li>
<li>An unordered set ≡ {a b c … z}.</li>
<li>s,s’:String; ss’ ≡ concatenation of s and s’.</li>
<li>‘«x»’ ≡ «x» is to be replaced by what ‘x’ represents.</li>
<li>‘□’ ≡ something is to be written here.</li>
<li>Let f : A → B: f(x) ≡ f x.</li>
<li><span class="math inline">x \cdot y</span> ≡ dot product of <span class="math inline">x</span> and <span class="math inline">y</span>.</li>
<li>Let f : A B … → C then f a : B … → C</li>
<li><span class="math inline">x_i</span> ≡ component indexed <span class="math inline">i</span> of <span class="math inline">x</span></li>
<li><span class="math inline">\blacksquare</span> ≡ end of computation</li>
</ul>
<h1 id="heaviside-step-function">Heaviside step function</h1>
<p><span class="math display">\displaystyle 
H : x \ge 0\mapsto H(x) = 1
</span></p>
<p><span class="math display">\displaystyle 
H : x &lt; 0 \mapsto H(x) = 0
</span></p>
<h1 id="perceptron">Perceptron</h1>
<p><span class="math display">\displaystyle 
\text{Perceptron}_w(x) \equiv H(w \cdot x)
</span></p>
<p>AND/OR/NOT/XOR perceptron</p>
<p>input → layers of perceptrons → outputs</p>
<p>Layers of perceptrons can simulate any computer programs.</p>
<p>Stacking units will let you model linearly inseparable data, impossible to do with regression models.</p>
<h1 id="supervised-learning">Supervised learning</h1>
<p>(Picture)</p>
<h1 id="deep-learning">Deep learning</h1>
<p>(Picture)</p>
<p><span class="math inline">\text{out}_{k-1,i}</span> output of node <span class="math inline">i</span> in layer <span class="math inline">k-1</span>. Together, they form the row vector: <span class="math inline">\underline{\text{out}_{k-1}}</span></p>
<p><span class="math inline">w_{ikj}</span> weight of node <span class="math inline">j</span> in layer <span class="math inline">k</span> for the output of node <span class="math inline">i</span> in layer <span class="math inline">k-1</span>. Gathering all weights on <span class="math inline">i</span> forms the column vector <span class="math inline">\underline{w_{kj}}</span>. Gathering for all nodes <span class="math inline">j</span> gives the matrix <span class="math inline">W_{k}</span>.</p>
<p>For an activation function <span class="math inline">\varphi</span>, <span class="math inline">\text{out}_{k,j} = \varphi(\underline{\text{out}_{k-1}} \cdot \underline{w_{kj}}) = \varphi(\underline{\text{in}_{k,j}})</span></p>
<p>Given an input <span class="math inline">\underline{x}</span>, the output of the network is <span class="math inline">\hat{y}</span> and the expected output is <span class="math inline">y</span>.</p>
<p>The error of the network <span class="math inline">E \equiv \frac{1}{2}(y - \hat{y})^2</span></p>
<p>The objective is to make the computer find itself <span class="math inline">W_{k}</span> ∀k such that it minimizes <span class="math inline">E</span>. A way to do that is for the computer to update <span class="math inline">w_{i,k,j}</span> ∀i,k,j like so: <span class="math inline">w&#39;_{i,k,j} = w_{i,k,j} + \Delta w_{i,k,j}</span> where <span class="math inline">\Delta w_{i,k,j} = - \eta \frac{\partial E}{\partial w_{i,k,j}}</span></p>
<h1 id="backpropagation">Backpropagation</h1>
<p>How to compute <span class="math inline">\frac{\partial E}{\partial w_{i,k,j}}</span> ? The intuition is: when one messes around with <span class="math inline">w_{i,k,j}</span> then <span class="math inline">\text{in}_{k+1,j}</span> jiggles a bit which makes <span class="math inline">\text{in}_{k+2,j}</span> jiggle and so on until the output layer where we can compute the error. From the end error and the quantitative dependency in between layers we can propagate backward the error and measure <span class="math inline">\frac{\partial E}{\partial w_{i,k,j}}</span>.</p>
<p>Let’s compute the output layer error:</p>
<p><span class="math display">\displaystyle 
\begin{aligned}
\frac{\partial E}{\partial w_{i,m}} &amp; \equiv 
\frac{\partial}{\partial w_{i,m}} \frac{1}{2}(y - \hat{y})^2 \equiv 
-(y - \hat{y})\frac{\partial}{\partial w_{i,m}}\hat{y} \equiv 
-(y - \hat{y})\frac{\partial}{\partial w_{i,m}} \varphi(\underline{\text{out}_{m-1}} \cdot \underline{w_{m}}) \\
\\
&amp; \equiv -(y - \hat{y})\varphi&#39;(\underline{\text{out}_{m-1}} \cdot \underline{w_{m}})\text{out}_{m-1,i} \\
\\
\delta_{m} &amp; \equiv -(y - \hat{y})\varphi&#39;(\underline{\text{out}_{m-1}} \cdot \underline{w_{m}}) \\
\\
\frac{\partial E}{\partial w_{i,m}} &amp; \equiv \delta_{m} \text{out}_{m-1,i} ~ \blacksquare
\end{aligned}
</span></p>
<p>Let’s compute the error from one layer to the other. The above intuition leads to:</p>
<p><span class="math display">\displaystyle 
\begin{aligned}
\frac{\partial E}{\partial w_{i,k,j}} &amp; \equiv 
\frac{\partial E}{\partial \underline{\text{in}_{k,j}}}\frac{\partial \underline{\text{in}_{k,j}}}{\partial w_{i,k,j}} 
\end{aligned}
</span></p>
<p>Second term:</p>
<p><span class="math display">\displaystyle 
\begin{aligned}
\frac{\partial \underline{\text{in}_{k,j}}}{\partial w_{i,k,j}} &amp; \equiv \text{out}_{k-1,i}
\end{aligned}
</span></p>
<p>First term: note that <span class="math inline">E \equiv E(\underline{\text{in}_{k+1,l}})</span> ∀l ≡ knowing the inputs of one layer let you compute all subsequente layers until the error, by construction.</p>
<p><span class="math display">\displaystyle 
\begin{aligned}
\frac{\partial E}{\partial \underline{\text{in}_{k,j}}} &amp; \equiv 
\delta_{k,j} \equiv 
\sum_l \delta_{k+1,l}{ \frac{\partial \underline{\text{in}_{k+1,l}}}{\partial \underline{\text{in}_{k,j}}}} \\
\\
\frac{\partial \underline{\text{in}_{k+1,l}}}{\partial \underline{\text{in}_{k,j}}} &amp; \equiv 
\frac{\partial \underline{\text{out}_{k}} \cdot \underline{w_{k+1,l}}}{\partial \underline{\text{in}_{k,j}}} \equiv 
\varphi&#39;(\text{in}_{k,j})w_{j,k+1,l} \\
\\
\frac{\partial E}{\partial \underline{\text{in}_{k,j}}} &amp; \equiv \delta_{k,j} \equiv \varphi&#39;(\text{in}_{k,j}) \times \sum_l \delta_{k+1,l} w_{j,k+1,l}
\end{aligned}
</span></p>
<p>In conclusion:</p>
<p><span class="math display">\displaystyle 
\frac{\partial E}{\partial w_{i,k,j}} \equiv \text{out}_{k-1,i} \delta_{k,j} \equiv \text{out}_{k-1,i}\varphi&#39;(\text{in}_{k,j}) \times (\sum_l \delta_{k+1,l} w_{j,k+1,l}) ~ \blacksquare
</span></p>
<p>In other words, to compute <span class="math inline">\frac{\partial E}{\partial w_{i,k,j}}</span> after a forward computation has been done, one needs the last error and compute from the last layers to the earliest until the first layer.</p>
<h1 id="practice">Practice</h1>
<p>The human intervention has been moved from optimizing the weights of the network to the more general optimization of the “shape” of the data where the automatic learning is supposed to happen.</p>
<p>E.g., the sigmoid will be more useful on a 0 centered set with a reasonable spread because the sigmoid is linear around 0 but not far from it. Far from zero, “everything looks the same” i.e. the derivative is close to 0.</p>
                                            </div>
                </div>
            </div>
        </div>
    </body>
</html>
