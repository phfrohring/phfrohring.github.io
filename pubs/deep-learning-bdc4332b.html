<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1"/>
        <title>Pierre-Henry Fröhring</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" type="text/css"/>
        <link rel="stylesheet" type="text/css" href="/css/style.css"/>
                <link rel="icon" type="image/png" href="/images/favicon.png"/>
                        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" /><script type="text/javascript">window.onload = function(){var mathElements = document.getElementsByClassName("math");
        for (var i=0; i < mathElements.length; i++)
        {
         var texText = mathElements[i].firstChild
         katex.render(texText.data, mathElements[i])
        }}
        </script>
            </head>

    <body>
        <div class="top-wrapper">
            <header class="top-header" title="By MartinThoma - Own work, CC0, https://commons.wikimedia.org/w/index.php?curid=45488162" style="background-image: url(/images/header-orig-deep-dream-0210.jpg);background-size: cover; background-position: center;"></header>

            <div class="columns_1">
                <nav class="navigation">
                    <ul class="list_of-predifined_publications">
                        <li class="predefined_publication"><a href="/pubs/curriculum-vitae-c5566488.html">CV</a></li>
                        <li class="predefined_publication"><a href="/">Search</a></li>
                    </ul>
                    <input class="card search_box" type="text" id="search_box" placeholder="some words   tag:some_tag">
                </nav>

                <div class="content">
                    <div class="publication-wrapper">
                                                <header>
                            <h1 class="title">Deep Learning</h1>
                                                        <p class="abstract">Notes on Deep Learning.</p>
                                                                                                                <p class="date">2017-08-03</p>
                                                                                                                      <p class="author"><a href="mailto:contact@pierrehenryfrohring.com">Pierre-Henry Fröhring</a></p>
                                                                                  </header>
                                                                        <nav id="TOC">
                            <ul>
                            <li><a href="#notations">Notations</a></li>
                            <li><a href="#heaviside-step-function">Heaviside step function</a></li>
                            <li><a href="#perceptron">Perceptron</a></li>
                            <li><a href="#deep-learning">Deep learning</a></li>
                            <li><a href="#gradient-descent">Gradient descent</a></li>
                            <li><a href="#practice">Practice</a></li>
                            </ul>
                        </nav>
                                                <h1 id="notations">Notations</h1>
<ul>
<li>‘abcdfegh ≡ X’ ⇔ ‘abcdfegh’ a.k.a. ‘X’.</li>
<li>‘A;B;C;… ⊢ E;F;G;…’ ≡ From assumptions A B C … we can conclude E F G ….</li>
<li>An ordered list ≡ [a,b,c, …, z].</li>
<li>An unordered list ≡ [a b c … z].</li>
<li>An ordered set ≡ {a,b,c,…,z}.</li>
<li>An unordered set ≡ {a b c … z}.</li>
<li>s,s’:String; ss’ ≡ concatenation of s and s’.</li>
<li>‘«x»’ ≡ «x» is to be replaced by what ‘x’ represents.</li>
<li>‘□’ ≡ something is to be written here.</li>
<li>Let f : A → B: f(x) ≡ f x.</li>
<li><span class="math inline">x \cdot y</span> ≡ dot product of <span class="math inline">x</span> and <span class="math inline">y</span>.</li>
<li>Let f : A B … → C then f a : B … → C</li>
<li><span class="math inline">x_i</span> ≡ component indexed <span class="math inline">i</span> of <span class="math inline">x</span></li>
</ul>
<h1 id="heaviside-step-function">Heaviside step function</h1>
<p><span class="math display">\displaystyle 
H : x \ge 0\mapsto H(x) = 1
</span></p>
<p><span class="math display">\displaystyle 
H : x &lt; 0 \mapsto H(x) = 0
</span></p>
<h1 id="perceptron">Perceptron</h1>
<p><span class="math display">\displaystyle 
\text{Perceptron}_w(x) \equiv H(w \cdot x)
</span></p>
<p>AND/OR/NOT/XOR perceptron</p>
<p>input → layers of perceptrons → outputs</p>
<p>Layers of perceptrons can simulate any computer programs.</p>
<p>Stacking units will let you model linearly inseparable data, impossible to do with regression models.</p>
<h1 id="deep-learning">Deep learning</h1>
<p>Input ≡ <span class="math inline">x</span> ≡ <span class="math inline">[x_1,\ldots,x_n]</span>, <span class="math inline">x_i \equiv [x_{i1},\ldots,x_{im}]</span></p>
<p>Expected ouput ≡ <span class="math inline">y</span> ≡ <span class="math inline">[y_1,\ldots,y_n]</span>, <span class="math inline">y_i \equiv [y_{i1},\ldots,y_{ik}]</span></p>
<p>Weights of the network ≡ <span class="math inline">w</span> ≡ <span class="math inline">[w_1,\ldots,w_m]</span>, weights of a node <span class="math inline">i</span> in the network ≡ <span class="math inline">w_i</span> ≡ <span class="math inline">[w_{i1},\ldots,w_{ik}]</span></p>
<p><span class="math inline">\hat{y}</span> ≡ <span class="math inline">\text{network}(x) \equiv \varphi \odot x \times w</span> where <span class="math inline">\times</span> is the matrix multiplication, <span class="math inline">\odot</span> is the pointwise multiplication of a function and a matrix.</p>
<p>Error comparing (SSE) <span class="math inline">\hat{y}</span> and <span class="math inline">y</span> ≡ <span class="math inline">E</span> ≡ <span class="math inline">\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^k (y_{ij} - \hat{y}_{ij})^2</span></p>
<p>Also possible: error (MSE) comparing <span class="math inline">\hat{y}</span> and <span class="math inline">y</span> for one output layer and <span class="math inline">n</span> input points ≡ <span class="math inline">E</span> ≡ <span class="math inline">\frac{1}{2n} \sum_{i=1}^n (y_{i} - \hat{y}_{i})^2</span></p>
<p>It’s impractical to modify the network by hand so that <span class="math inline">E</span> is minimized. It may be done by the computer by looking at how <span class="math inline">E</span> changes wrt some parameters of the network then updating the parameters so that <span class="math inline">E</span> is minimized until no more optimization can occur.</p>
<h1 id="gradient-descent">Gradient descent</h1>
<p>Assuming the above network, <span class="math inline">E</span> and that <span class="math inline">\varphi</span> is continuous differentiable we can conclude that the computer can minimize <span class="math inline">E</span> by updating <span class="math inline">w</span> one component at a time like so: <span class="math inline">w_{op}&#39; \equiv w_{op} + \Delta w_{op}</span>, <span class="math inline">\eta</span> ≡ learning rate, <span class="math inline">\Delta w_{op} \equiv - \eta \frac{\partial E}{\partial w_{op}}</span></p>
<p>Let’s derive <span class="math inline">w_{op}&#39; \equiv w_{op} + \Delta w_{op}</span> in terms of operations a computer may perform:</p>
<p><span class="math display">\displaystyle 
\frac{\partial E}{\partial w_{op}} \equiv -\sum_{i,j}(y_{ij} - \hat{y}_{ij}) \frac{\partial \hat{y}_{ij}}{\partial w_{op}}
</span></p>
<p><span class="math display">\displaystyle 
\frac{\partial \hat{y}_{ij}}{\partial w_{op}} \equiv \frac{\partial \varphi(x_i \cdot w_j)}{\partial w_{op}} \equiv \varphi&#39;(x_i \cdot w_j) \frac{\partial}{\partial w_{op}} x_i \cdot w_j
</span></p>
<p><span class="math display">\displaystyle 
\frac{\partial}{\partial w_{op}} x_i \cdot w_j \equiv \frac{\partial}{\partial w_{op}} x_i \cdot w_o \equiv x_{ip}
</span></p>
<p><span class="math display">\displaystyle 
\varphi&#39;(x_i \cdot w_j) \frac{\partial}{\partial w_{op}} x_i \cdot w_j \equiv \varphi&#39;(x_i \cdot w_o) x_{ip}
</span></p>
<p><span class="math display">\displaystyle 
\frac{\partial E}{\partial w_{op}} \equiv -\sum_{i}(y_{io} - \hat{y}_{io}) \varphi&#39;(x_i \cdot w_o) x_{ip}
</span></p>
<p><span class="math display">\displaystyle 
\delta_{io} \equiv (y_{io} - \hat{y}_{io}) \varphi&#39;(x_i \cdot w_o)
</span></p>
<p><span class="math display">\displaystyle 
\Delta w_{op} \equiv \eta \sum_{i}\delta_{io} x_{ip} \equiv \eta(\delta_{o} \cdot x_{p})
</span></p>
<p><span class="math display">\displaystyle 
w_{op}&#39; \equiv w_{op} + \eta (\delta_{o} \cdot x_{p})
</span></p>
<p>A commonly used <span class="math inline">\varphi</span> function is the sigmoid function: <span class="math inline">\text{Sigmoid}(z) \equiv S(z) \equiv \frac{1}{1 - e^{-z}}</span></p>
<p>Local optimum may trap gradient descents.</p>
<h1 id="practice">Practice</h1>
<p>The human intervention has been moved from optimizing the weights of the network to the more general optimization of the “shape” of the data where the automatic learning is supposed to happen.</p>
<p>E.g., the sigmoid will be more useful on a 0 centered set with a reasonable spread because the sigmoid is linear around 0 but not far from it. Far from zero, “everything looks the same” i.e. the derivative is close to 0.</p>
                                            </div>
                </div>
            </div>
        </div>
    </body>
</html>
