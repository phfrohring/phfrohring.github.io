<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1"/>
        <title>Pierre-Henry Fröhring</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css" type="text/css"/>
        <link rel="stylesheet" type="text/css" href="/css/style.css"/>
                <link rel="icon" type="image/png" href="/images/favicon.png"/>
                        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" /><script type="text/javascript">window.onload = function(){var mathElements = document.getElementsByClassName("math");
        for (var i=0; i < mathElements.length; i++)
        {
         var texText = mathElements[i].firstChild
         katex.render(texText.data, mathElements[i])
        }}
        </script>
            </head>

    <body>
        <div class="top-wrapper">
            <header class="top-header" title="By MartinThoma - Own work, CC0, https://commons.wikimedia.org/w/index.php?curid=45488162" style="background-image: url(/images/header-orig-deep-dream-0210.jpg);background-size: cover; background-position: center;"></header>

            <div class="columns_1">
                <nav class="navigation">
                    <ul class="list_of-predifined_publications">
                        <li class="predefined_publication"><a href="/pubs/curriculum-vitae-c5566488.html">CV</a></li>
                        <li class="predefined_publication"><a href="/">Search</a></li>
                    </ul>
                    <input class="card search_box" type="text" id="search_box" placeholder="some words   tag:some_tag">
                </nav>

                <div class="content">
                    <div class="publication-wrapper">
                                                <header>
                            <h1 class="title">Deep Learning</h1>
                                                        <p class="abstract">Soon.</p>
                                                                                                                <p class="date">2017-08-03</p>
                                                                                                                      <p class="author"><a href="mailto:contact@pierrehenryfrohring.com">Pierre-Henry Fröhring</a></p>
                                                                                  </header>
                                                                        <nav id="TOC">
                            <ul>
                            <li><a href="#deep-learning">Deep Learning</a></li>
                            </ul>
                        </nav>
                                                <h1 id="deep-learning">Deep Learning</h1>
<p>Gradient Descent</p>
<p>Heaviside step function</p>
<p>Perceptrons</p>
<p><span class="math display">\displaystyle 
x,w : \mathbb{Q}^n;~g(x,w) :\equiv x \cdot w
</span></p>
<p><span class="math display">\displaystyle 
g(x) \geq 0 \Rightarrow f(x) :\equiv 1
</span></p>
<p><span class="math display">\displaystyle 
g(x) &lt; 0 \Rightarrow f(x) :\equiv 0
</span></p>
<p>AND/OR/NOT/XOR perceptron</p>
<p>input → layers of perceptrons → outputs</p>
<p>Layers of perceptrons can simulate any computer programs.</p>
<p><span class="math display">\displaystyle 
z : \mathbb{Q};~s(z) :\equiv \frac{1}{1 - e^{-z}}
</span></p>
<p>Stacking units will let you model linearly inseparable data, impossible to do with regression models.</p>
<p>Once you start using activation functions that are continuous and differentiable, it’s possible to train the network using gradient descent.</p>
<p>Assume that <span class="math inline">\varphi</span> : continuous differentiable.</p>
<p><span class="math display">\displaystyle 
\hat{y} :\equiv \varphi \circ g
</span></p>
<p><span class="math display">\displaystyle 
\hat{y}^i :\equiv \varphi \circ g~x_i
</span></p>
<p><span class="math display">\displaystyle 
\hat{y}^i_j :\equiv (\varphi \circ g~x_i)_j
</span></p>
<p><span class="math display">\displaystyle 
y,\hat{y} : \mathbb{Q};~E :\equiv \frac{1}{2} \sum_{i} \sum_{j} (y_j^i - \hat{y}_j^i)^2
</span></p>
<p><span class="math inline">\eta:\mathbb{Q}</span> is called the learning rate.</p>
<p><span class="math display">\displaystyle 
\Delta w_i :\equiv - \eta \frac{\partial E}{\partial w_i}
</span></p>
<p><span class="math display">\displaystyle 
w_i&#39; :\equiv w_i + \Delta w_i
</span></p>
<p><span class="math display">\displaystyle 
(f \circ g)&#39; = f&#39; \circ g \cdot g
</span></p>
<p><span class="math display">\displaystyle 
\frac{\partial E}{\partial w_k} = - \sum_{i} \sum_{j} (y_j^i - \hat{y}_j^i) \varphi&#39; (g(x^i)) x_k^i 
</span></p>
<p><span class="math display">\displaystyle 
\delta^i :\equiv \sum_{j} (y_j^i - \hat{y}_j^i) \varphi&#39; (g(x^i))
</span></p>
<p><span class="math display">\displaystyle 
\frac{\partial E}{\partial w_k} = - \sum_{i} \delta^i x_k^i 
</span></p>
<p><span class="math display">\displaystyle 
w_k&#39; :\equiv w_k + \eta \sum_{i} \delta^i x_k^i 
</span></p>
                                            </div>
                </div>
            </div>
        </div>
    </body>
</html>
